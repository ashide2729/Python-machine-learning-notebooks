{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "Wiki Definition:\n",
    "\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.\n",
    "\n",
    "In short NLTK is a NLP(Natural Language Processing) library using which one can acheive text processng and handling for further usage in areas like speech recognition, classification, clustering, text analytics, sentiment analysis and much more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topics that we will be going through the notebook are:\n",
    "- nltk corpus\n",
    "- Words usage count and context\n",
    "- Dictionary definitions\n",
    "- Punctuations and Stop words\n",
    "- Stemming and Lemmatization\n",
    "- Sentence, word and paragraph tokenizer\n",
    "- POS(Parts of speech) tagging\n",
    "- Named entity recognition\n",
    "\n",
    "Apart from there there are other powerful features in NLTK but with all the above mentioned topics you will be able to understand the NLTK and get started with NLTK very easily "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the library the user first have to install the library using the below command can help \n",
    "!pip3 install nltk\n",
    "\n",
    "### Now that you have installed the library you just have to import in your project to use it \n",
    "import nltk\n",
    "\n",
    "### This command provides the user with window to install the required packages from NLTK\n",
    "nltk.download()\n",
    "\n",
    "### The download window will show options like which we will discuss later in this notebook\n",
    "collections, corpora, models, package\n",
    "\n",
    "### For example to download the \"punkt\" package\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are all setup let's start with our first NLTK code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to import the nltk book to get some sample texts and sentences to start with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "<class 'nltk.text.Text'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "# text1, text2, text3, ..... are some texts given by book to work with\n",
    "\n",
    "print(type(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's analyze what we have in the texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "\n",
      "<Text: Moby Dick by Herman Melville 1851>\n",
      "\n",
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar']\n",
      "\n",
      "Number of words in text1 are 260819\n",
      "\n",
      "Number of unique words in text1 are 19317\n"
     ]
    }
   ],
   "source": [
    "texts()\n",
    "print('')\n",
    "print(text1)\n",
    "print('')\n",
    "print(text1[:20])\n",
    "print('')\n",
    "print('Number of words in text1 are',len(text1))\n",
    "print('')\n",
    "print('Number of unique words in text1 are',len(set(text1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's analyze what we have in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n",
      "\n",
      "['THE', 'suburb', 'of', 'Saffron', 'Park', 'lay', 'on', 'the', 'sunset', 'side', 'of', 'London', ',', 'as', 'red', 'and', 'ragged', 'as', 'a', 'cloud', 'of', 'sunset', '.'] \n",
      "\n",
      "Number of words in sent9 are 23 \n",
      "\n",
      "Number of unique words in sent9 are 19\n"
     ]
    }
   ],
   "source": [
    "sents()\n",
    "print('')\n",
    "print(sent9,'\\n')\n",
    "\n",
    "print('Number of words in sent9 are',len(sent9),'\\n')\n",
    "\n",
    "print('Number of unique words in sent9 are',len(set(sent9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Corpus\n",
    "Now for better understanding let's get some actual long texts to work with\n",
    "To achieve that we have nltk corpus which is a huge collection of text files to work with to get hold of the NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text extraction\n",
    "Now that we have the collection let's fetch a specific file to work with \n",
    "Here we will be getting the words, sentences, and paras from the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', ...]\n",
      "Melville Moby Dick has 260819 words\n",
      "Melville Moby Dick has 10059 sentences\n",
      "Melville Moby Dick has 2793 paragraphs \n",
      "\n",
      "The 102nd para in Melville Moby Dick is [['Now', ',', 'when', 'I', 'say', 'that', 'I', 'am', 'in', 'the', 'habit', 'of', 'going', 'to', 'sea', 'whenever', 'I', 'begin', 'to', 'grow', 'hazy', 'about', 'the', 'eyes', ',', 'and', 'begin', 'to', 'be', 'over', 'conscious', 'of', 'my', 'lungs', ',', 'I', 'do', 'not', 'mean', 'to', 'have', 'it', 'inferred', 'that', 'I', 'ever', 'go', 'to', 'sea', 'as', 'a', 'passenger', '.'], ['For', 'to', 'go', 'as', 'a', 'passenger', 'you', 'must', 'needs', 'have', 'a', 'purse', ',', 'and', 'a', 'purse', 'is', 'but', 'a', 'rag', 'unless', 'you', 'have', 'something', 'in', 'it', '.'], ['Besides', ',', 'passengers', 'get', 'sea', '-', 'sick', '--', 'grow', 'quarrelsome', '--', 'don', \"'\", 't', 'sleep', 'of', 'nights', '--', 'do', 'not', 'enjoy', 'themselves', 'much', ',', 'as', 'a', 'general', 'thing', ';--', 'no', ',', 'I', 'never', 'go', 'as', 'a', 'passenger', ';', 'nor', ',', 'though', 'I', 'am', 'something', 'of', 'a', 'salt', ',', 'do', 'I', 'ever', 'go', 'to', 'sea', 'as', 'a', 'Commodore', ',', 'or', 'a', 'Captain', ',', 'or', 'a', 'Cook', '.'], ['I', 'abandon', 'the', 'glory', 'and', 'distinction', 'of', 'such', 'offices', 'to', 'those', 'who', 'like', 'them', '.'], ['For', 'my', 'part', ',', 'I', 'abominate', 'all', 'honourable', 'respectable', 'toils', ',', 'trials', ',', 'and', 'tribulations', 'of', 'every', 'kind', 'whatsoever', '.'], ['It', 'is', 'quite', 'as', 'much', 'as', 'I', 'can', 'do', 'to', 'take', 'care', 'of', 'myself', ',', 'without', 'taking', 'care', 'of', 'ships', ',', 'barques', ',', 'brigs', ',', 'schooners', ',', 'and', 'what', 'not', '.'], ['And', 'as', 'for', 'going', 'as', 'cook', ',--', 'though', 'I', 'confess', 'there', 'is', 'considerable', 'glory', 'in', 'that', ',', 'a', 'cook', 'being', 'a', 'sort', 'of', 'officer', 'on', 'ship', '-', 'board', '--', 'yet', ',', 'somehow', ',', 'I', 'never', 'fancied', 'broiling', 'fowls', ';--', 'though', 'once', 'broiled', ',', 'judiciously', 'buttered', ',', 'and', 'judgmatically', 'salted', 'and', 'peppered', ',', 'there', 'is', 'no', 'one', 'who', 'will', 'speak', 'more', 'respectfully', ',', 'not', 'to', 'say', 'reverentially', ',', 'of', 'a', 'broiled', 'fowl', 'than', 'I', 'will', '.'], ['It', 'is', 'out', 'of', 'the', 'idolatrous', 'dotings', 'of', 'the', 'old', 'Egyptians', 'upon', 'broiled', 'ibis', 'and', 'roasted', 'river', 'horse', ',', 'that', 'you', 'see', 'the', 'mummies', 'of', 'those', 'creatures', 'in', 'their', 'huge', 'bake', '-', 'houses', 'the', 'pyramids', '.']]\n"
     ]
    }
   ],
   "source": [
    "moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
    "print(moby_dick)\n",
    "print('Melville Moby Dick has',len(moby_dick),'words')\n",
    "moby_dick_sentences = gutenberg.sents('melville-moby_dick.txt')\n",
    "print('Melville Moby Dick has',len(moby_dick_sentences),'sentences')\n",
    "moby_dick_paras = gutenberg.paras('melville-moby_dick.txt')\n",
    "print('Melville Moby Dick has',len(moby_dick_paras),'paragraphs','\\n')\n",
    "\n",
    "print('The 102nd para in Melville Moby Dick is',moby_dick_paras[102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific word count and context\n",
    "A user can find the number of usage of a specific word in the text and also in what context that word was used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of word Moby in Melville Moby Dick is 84 \n",
      "\n",
      "Displaying 25 of 84 matches:\n",
      "[ Moby Dick by Herman Melville 1851 ] ETYMO\n",
      "hale must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab \n",
      "e same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye know \n",
      "all . Death and devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Mo\n",
      "en , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab \n",
      "by Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab ,\" said Starb\n",
      "r . \" Captain Ahab , I have heard of Moby Dick -- but it was not Moby Dick tha\n",
      "heard of Moby Dick -- but it was not Moby Dick that took off thy leg ?\" \" Who \n",
      "aye , my hearties all round ; it was Moby Dick that dismasted me ; Moby Dick t\n",
      "it was Moby Dick that dismasted me ; Moby Dick that brought me to this dead st\n",
      " the white whale ; a sharp lance for Moby Dick !\" \" God bless ye ,\" he seemed \n",
      "e the white whale ? art not game for Moby Dick ?\" \" I am game for his crooked \n",
      "athful whaleboat ' s bow -- Death to Moby Dick ! God hunt us all , if we do no\n",
      " God hunt us all , if we do not hunt Moby Dick to his death !\" The long , barb\n",
      " no bowels to feel fear ! CHAPTER 41 Moby Dick . I , Ishmael , was one of that\n",
      "l individualizing tidings concerning Moby Dick . It was hardly to be doubted ,\n",
      "uestion must have been no other than Moby Dick . Yet as of late the Sperm Whal\n",
      "y accident ignorantly gave battle to Moby Dick ; such hunters , perhaps , for \n",
      "lating and piling their terrors upon Moby Dick ; those things had gone far to \n",
      "agencies , which eventually invested Moby Dick with new terrors unborrowed fro\n",
      "fishermen recalled , in reference to Moby Dick , the earlier days of the Sperm\n",
      "e things were ready to give chase to Moby Dick ; and a still greater number wh\n",
      "ned , was the unearthly conceit that Moby Dick was ubiquitous ; that he had ac\n",
      "r in their superstitions ; declaring Moby Dick not only ubiquitous , but immor\n",
      "kle - shaped lower jaw beneath him , Moby Dick had reaped away Ahab ' s leg , \n",
      "The contexts in which the word Moby is used in Melville Moby Dick are None\n"
     ]
    }
   ],
   "source": [
    "print('The count of word Moby in Melville Moby Dick is',moby_dick.count('Moby'),'\\n')\n",
    "\n",
    "#Conversion to text object is required for context finding\n",
    "print('The contexts in which the word Moby is used in Melville Moby Dick are',Text(moby_dick).concordance('Moby'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency and operations\n",
    "We can find the count of each word alongwith determining most common words, words based on length and occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317\n",
      "\n",
      "Top 10 most frequent occurances are:\n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)] \n",
      "\n",
      "Top 5 words with length greater than 15 are:\n",
      "[('apprehensiveness', 4), ('indiscriminately', 3), ('comprehensiveness', 3), ('superstitiousness', 2), ('circumnavigating', 2)] \n",
      "\n",
      "Top 5 words occuring only once are:\n",
      "[('Herman', 1), ('Melville', 1), (']', 1), ('ETYMOLOGY', 1), ('Late', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_count = nltk.FreqDist(moby_dick)\n",
    "print(len(words_count))\n",
    "print('')\n",
    "print('Top 10 most frequent occurances are:')\n",
    "print(words_count.most_common(10),'\\n')\n",
    "long_words = [word for word in words_count.most_common() if len(word[0])>15][:5]\n",
    "print('Top 5 words with length greater than 15 are:')\n",
    "print(long_words,'\\n')\n",
    "less_freq_words = [word for word in words_count.most_common() if word[1]==1][:5]\n",
    "print('Top 5 words occuring only once are:')\n",
    "print(less_freq_words,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a plot to show occurance of word/s in the whole text graphically \n",
    "Text(moby_dick).dispersion_plot([\"Commodore\",\"Moby\",\"Melville\",\"Ahab\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary definition\n",
    "Now nltk provides wordnet module in order to get the dictionary understanding of the words present in our text \n",
    "where similar to a dictionary we can easily find the word's meaning, pos, example sentence, synonyms etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word is: life.n.01\n",
      "The part of speech is: n\n",
      "Word definition: a characteristic state or mode of living\n",
      "Word usage example: ['social life', 'city life', 'real life']\n",
      "Dict abstract term: [Synset('being.n.01')]\n",
      "Dict specific term: [Synset('actuality.n.01'), Synset('animation.n.01'), Synset('coexistence.n.01'), Synset('eternity.n.02'), Synset('life.n.01'), Synset('life.n.03'), Synset('possibility.n.02'), Synset('preexistence.n.01'), Synset('presence.n.01'), Synset('subsistence.n.03'), Synset('transcendence.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "word_dict = wn.synsets('life')[0]\n",
    "print('The word is:',word_dict.name())\n",
    "print('The part of speech is:',word_dict.pos())\n",
    "print('Word definition:',word_dict.definition())\n",
    "print('Word usage example:',word_dict.examples())\n",
    "print('Dict abstract term:',word_dict.hypernyms())\n",
    "print('Dict specific term:',word_dict.hypernyms()[0].hyponyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation and stop words\n",
    "Punctuations in a sentence are not usually required and are required to be removed which can be easily done using string \n",
    "library with nltk\n",
    "\n",
    "Stop words like is, am, are, that doesn't convey much of a meaning and usually are not required during analysis so to remove\n",
    "them nltk provides stopwords module which is a collection different stopwords that too with support in different languages\n",
    "\n",
    "Removing punctuations and stopwords also help reduce the amount of text for analysis by significant amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Words before removing punctuations 260819\n",
      "Words after removing punctuations 221767 \n",
      "\n",
      "Stopwords: \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "\n",
      "Words after removing punctuations and stopwords 122226\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print('Punctuations:',punctuation)\n",
    "print('Words before removing punctuations',len(moby_dick))\n",
    "moby_dick_without_punc = [ word for word in moby_dick if word not in punctuation]\n",
    "print('Words after removing punctuations',len(moby_dick_without_punc),'\\n')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "print('Stopwords:','\\n',sw,'\\n')\n",
    "#You might want to lowercase all the words first to achieve better results\n",
    "moby_dick_without_punc_sw = [ word for word in moby_dick_without_punc if word not in sw]\n",
    "print('Words after removing punctuations and stopwords',len(moby_dick_without_punc_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "Stemming is the process of converting the words to their root forms by removing e, es, ed etc. from the words for better analysis \n",
    "Lemmatization on the other hand is conversion to root form but the converted will be an actual grammar word\n",
    "\n",
    "Various modules provided by nltk for stemming are\n",
    "PorterStemmer - is most frequently used \n",
    "LancasterStemmer - it is a very aggressive stemmer which sometimes result in meaningless words\n",
    "SnowballStemmer - it is stemmer which provides user with options to manipulate stemming and it also has support for multiple languages\n",
    "WordNetLemmatizer - it converts the word to the root resukting in a proper grammer word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word    Porter stem word     Lancaster stem word     Lemma word     Snowball stem word\n",
      "--  --  --  --  --  --  --  --  --\n",
      "threadbare  --  threadbar  --  threadbare  --  threadbare  --  threadbar\n",
      "coat  --  coat  --  coat  --  coat  --  coat\n",
      "heart  --  heart  --  heart  --  heart  --  heart\n",
      "body  --  bodi  --  body  --  body  --  bodi\n",
      "brain  --  brain  --  brain  --  brain  --  brain\n",
      "I  --  I  --  i  --  I  --  i\n",
      "see  --  see  --  see  --  see  --  see\n",
      "He  --  He  --  he  --  He  --  he\n",
      "ever  --  ever  --  ev  --  ever  --  ever\n",
      "dusting  --  dust  --  dust  --  dusting  --  dust\n",
      "old  --  old  --  old  --  old  --  old\n",
      "lexicons  --  lexicon  --  lexicon  --  lexicon  --  lexicon\n",
      "grammars  --  grammar  --  gramm  --  grammar  --  grammar\n",
      "queer  --  queer  --  que  --  queer  --  queer\n",
      "handkerchief  --  handkerchief  --  handkerchief  --  handkerchief  --  handkerchief\n",
      "mockingly  --  mockingli  --  mock  --  mockingly  --  mock\n",
      "embellished  --  embellish  --  embel  --  embellished  --  embellish\n",
      "gay  --  gay  --  gay  --  gay  --  gay\n",
      "flags  --  flag  --  flag  --  flag  --  flag\n",
      "known  --  known  --  known  --  known  --  known\n",
      "nations  --  nation  --  nat  --  nation  --  nation\n",
      "world  --  world  --  world  --  world  --  world\n",
      "He  --  He  --  he  --  He  --  he\n",
      "loved  --  love  --  lov  --  loved  --  love\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "snowball = SnowballStemmer('english')\n",
    "print('Word  ',' Porter stem word    ', 'Lancaster stem word    ', 'Lemma word    ', 'Snowball stem word')\n",
    "for word in moby_dick_without_punc_sw[15:40]:\n",
    "    print(word, ' -- ', stemmer.stem(word), ' -- ', lancaster.stem(word), ' -- ', lemma.lemmatize(word), ' -- ', snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of seperating tokens(parts) out of a group of text which can be in the form of words group, \n",
    "sentences group etc.\n",
    "\n",
    "NLTK provides modules like sent_tokenize, word_tokenize, blankline_tokenize to tokenize the text into sentences, words, and line seperated paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph tokenized into sentences: \n",
      " ['Welcome!', 'Hey Underdog.', \"You're here to learn NLTK.\", '123.45 is your number.', 'Questions?', 'Post it here.'] \n",
      "\n",
      "The paragraph tokenized into words; \n",
      " ['Welcome', '!', 'Hey', 'Underdog', '.', 'You', \"'re\", 'here', 'to', 'learn', 'NLTK', '.', '123.45', 'is', 'your', 'number', '.', 'Questions', '?', 'Post', 'it', 'here', '.'] \n",
      "\n",
      "The paragraph tokenized into new line seperated paras: \n",
      " [\"Welcome! Hey Underdog. You're here to learn NLTK. 123.45 is your number. Questions? Post it here.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, blankline_tokenize\n",
    "\n",
    "paragraph = \"Welcome! Hey Underdog. You're here to learn NLTK. 123.45 is your number. Questions? Post it here.\"\n",
    "\n",
    "print(\"The paragraph tokenized into sentences:\",'\\n',sent_tokenize(paragraph),'\\n')\n",
    "\n",
    "print(\"The paragraph tokenized into words;\",'\\n',word_tokenize(paragraph),'\\n')\n",
    "\n",
    "print(\"The paragraph tokenized into new line seperated paras:\",'\\n',blankline_tokenize(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token grams\n",
    "Words can tokenized into groups of words associations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams of sentence: \n",
      " [('Welcome', '!'), ('!', 'Hey'), ('Hey', 'Underdog'), ('Underdog', '.'), ('.', 'You'), ('You', \"'re\"), (\"'re\", 'here'), ('here', 'to'), ('to', 'learn'), ('learn', 'NLTK'), ('NLTK', '.'), ('.', '123.45'), ('123.45', 'is'), ('is', 'your'), ('your', 'number'), ('number', '.'), ('.', 'Questions'), ('Questions', '?'), ('?', 'Post'), ('Post', 'it'), ('it', 'here'), ('here', '.')] \n",
      "\n",
      "Trigrams of sentence: \n",
      " [('Welcome', '!', 'Hey'), ('!', 'Hey', 'Underdog'), ('Hey', 'Underdog', '.'), ('Underdog', '.', 'You'), ('.', 'You', \"'re\"), ('You', \"'re\", 'here'), (\"'re\", 'here', 'to'), ('here', 'to', 'learn'), ('to', 'learn', 'NLTK'), ('learn', 'NLTK', '.'), ('NLTK', '.', '123.45'), ('.', '123.45', 'is'), ('123.45', 'is', 'your'), ('is', 'your', 'number'), ('your', 'number', '.'), ('number', '.', 'Questions'), ('.', 'Questions', '?'), ('Questions', '?', 'Post'), ('?', 'Post', 'it'), ('Post', 'it', 'here'), ('it', 'here', '.')] \n",
      "\n",
      "Ngrams of sentence: \n",
      " [('Welcome', '!', 'Hey', 'Underdog', '.'), ('!', 'Hey', 'Underdog', '.', 'You'), ('Hey', 'Underdog', '.', 'You', \"'re\"), ('Underdog', '.', 'You', \"'re\", 'here'), ('.', 'You', \"'re\", 'here', 'to'), ('You', \"'re\", 'here', 'to', 'learn'), (\"'re\", 'here', 'to', 'learn', 'NLTK'), ('here', 'to', 'learn', 'NLTK', '.'), ('to', 'learn', 'NLTK', '.', '123.45'), ('learn', 'NLTK', '.', '123.45', 'is'), ('NLTK', '.', '123.45', 'is', 'your'), ('.', '123.45', 'is', 'your', 'number'), ('123.45', 'is', 'your', 'number', '.'), ('is', 'your', 'number', '.', 'Questions'), ('your', 'number', '.', 'Questions', '?'), ('number', '.', 'Questions', '?', 'Post'), ('.', 'Questions', '?', 'Post', 'it'), ('Questions', '?', 'Post', 'it', 'here'), ('?', 'Post', 'it', 'here', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "para_bigram = list(bigrams((word_tokenize(paragraph))))\n",
    "print('Bigrams of sentence:','\\n',para_bigram,'\\n')\n",
    "para_trigram = list(trigrams((word_tokenize(paragraph))))\n",
    "print('Trigrams of sentence:','\\n',para_trigram,'\\n')\n",
    "para_ngram = list(ngrams((word_tokenize(paragraph)),5))\n",
    "print('Ngrams of sentence:','\\n',para_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS_TAG\n",
    "\n",
    "POS_TAG helps determine the part of speech the word belongs which can be helpful in analysis and projects\n",
    "POS_TAG provides support for different languages which can be accessed using tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and their corresponding parts of speech: \n",
      " [('Welcome', 'JJ'), ('!', '.'), ('Hey', 'NNP'), ('Underdog', 'NNP'), ('.', '.'), ('You', 'PRP'), (\"'re\", 'VBP'), ('here', 'RB'), ('to', 'TO'), ('learn', 'VB'), ('NLTK', 'NNP'), ('.', '.'), ('123.45', 'CD'), ('is', 'VBZ'), ('your', 'PRP$'), ('number', 'NN'), ('.', '.'), ('Questions', 'NNS'), ('?', '.'), ('Post', 'VB'), ('it', 'PRP'), ('here', 'RB'), ('.', '.')] \n",
      "\n",
      "Words and their corresponding parts of speech as universally accepted POS: \n",
      " [('Welcome', 'ADJ'), ('!', '.'), ('Hey', 'NOUN'), ('Underdog', 'NOUN'), ('.', '.'), ('You', 'PRON'), (\"'re\", 'VERB'), ('here', 'ADV'), ('to', 'PRT'), ('learn', 'VERB'), ('NLTK', 'NOUN'), ('.', '.'), ('123.45', 'NUM'), ('is', 'VERB'), ('your', 'PRON'), ('number', 'NOUN'), ('.', '.'), ('Questions', 'NOUN'), ('?', '.'), ('Post', 'VERB'), ('it', 'PRON'), ('here', 'ADV'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "print('Words and their corresponding parts of speech:','\\n', pos_tag(word_tokenize(paragraph)),'\\n')\n",
    "print('Words and their corresponding parts of speech as universally accepted POS:','\\n', pos_tag(word_tokenize(paragraph), tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition(NER)\n",
    "\n",
    "NLTK provides ne_chunk module to help determine the different named entities present in the sentence which can be name, location, organisation etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Jeff/NNP)\n",
      "  (ORGANIZATION Bezos/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  (ORGANIZATION CEO/NN of/IN Amazon/NNP)\n",
      "  with/IN\n",
      "  it/PRP\n",
      "  's/VBZ\n",
      "  headquaters/NNS\n",
      "  in/IN\n",
      "  (GPE Seattle/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "sentence = \"Jeff Bezos is the CEO of Amazon with it's headquaters in Seattle.\"\n",
    "\n",
    "tokenized = word_tokenize(sentence)\n",
    "pos_tokenized = pos_tag(tokenized)\n",
    "print(ne_chunk(pos_tokenized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
